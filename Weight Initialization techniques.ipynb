{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4lqZyFYKwXsrVTKs2wU3q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-KfwiTgEZGYu"},"outputs":[],"source":["#1.What is the vanishing gradient problem in deep neural networks? How does it affect training."]},{"cell_type":"code","source":["Vanishing Gradient Problem\n","The vanishing gradient problem occurs when gradients used to update the weights in deep neural networks become extremely small. This often happens in networks with many layers, particularly when activation functions like sigmoid or tanh are used.\n","\n","Impact on Training\n","Slow Learning: Small gradients result in very tiny weight updates, slowing down the learning process significantly.\n","\n","Difficulty in Training Deep Networks: Earlier layers (closer to the input) receive minimal updates, hindering their ability to learn essential features, making it hard to train very deep networks.\n","\n","Poor Convergence: The network might struggle to converge to an optimal solution, resulting in subpar performance.\n","\n","In summary, the vanishing gradient problem can severely impede the training of deep neural networks by making it hard for them to learn effectively."],"metadata":{"id":"UrBb-hRQfvyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2.& Explain how Xavier initialization addresses the vanishing gradient problem&"],"metadata":{"id":"CSyBivptfvu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xavier Initialization\n","Xavier Initialization (also known as Glorot Initialization) is a method used to initialize the weights of neural networks to help mitigate the vanishing gradient problem. Here‚Äôs how it addresses the issue:\n","\n","Balanced Variance: Xavier initialization sets the weights so that the variance of the outputs of each layer is equal to the variance of its inputs. This helps in maintaining the flow of gradients throughout the network.\n","\n","Scalability: The weights are initialized based on the size of the previous layer. Specifically, they are drawn from a distribution with zero mean and a variance of\n","2\n","ùëõ\n","ùëñ\n","ùëõ\n","+\n","ùëõ\n","ùëú\n","ùë¢\n","ùë°\n",", where\n","ùëõ\n","ùëñ\n","ùëõ\n"," and\n","ùëõ\n","ùëú\n","ùë¢\n","ùë°\n"," are the number of input and output units, respectively. This keeps the weights from becoming too large or too small, preventing the gradients from vanishing or exploding.\n","\n","Impact\n","Stable Gradient Flow: By maintaining the variance of activations and gradients, Xavier initialization ensures that gradients neither vanish nor explode, enabling stable and efficient training of deep networks.\n","\n","In summary, Xavier initialization helps in keeping the gradients within a reasonable range, thereby addressing the vanishing gradient problem and facilitating the training of deep neural networks."],"metadata":{"id":"3xlwg82AfvrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3. What are some common activation functions that are prone to causing vanishing gradients."],"metadata":{"id":"Wpq7ZHGPfvok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Common Activation Functions Prone to Vanishing Gradients\n","Sigmoid Function:\n","\n","Range: Outputs values between 0 and 1.\n","\n","Issue: For very large or very small input values, the gradients become extremely small, leading to the vanishing gradient problem.\n","\n","Tanh Function:\n","\n","Range: Outputs values between -1 and 1.\n","\n","Issue: Similar to the sigmoid function, the gradients approach zero for inputs that are far from zero, causing the vanishing gradient problem.\n","\n","These activation functions can cause the gradients to become very small, making it difficult to train deep neural networks effectively."],"metadata":{"id":"RZQFVJ7NfvlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.Define the exploding gradient problem in deep neural networks. How does it impact training."],"metadata":{"id":"ysRtV9Hafvhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Exploding Gradient Problem\n","The exploding gradient problem occurs in deep neural networks when gradients grow exponentially during backpropagation. This issue is typically encountered in networks with many layers, particularly when using high learning rates or poorly initialized weights.\n","\n","Impact on Training\n","Instability: Extremely large gradients can cause the model parameters to update excessively, leading to unstable training.\n","\n","Divergence: The network's loss can increase instead of decrease, causing the model to diverge and fail to learn effectively.\n","\n","Model Performance: The overall performance and accuracy of the model can suffer, as it may fail to converge to an optimal solution.\n","\n","In summary, the exploding gradient problem can severely disrupt the training of deep neural networks by making them unstable and hindering their learning process.\n"],"metadata":{"id":"QznzmbxAfvLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5.What is the role of proper weight initialization in training deep neural networks."],"metadata":{"id":"bimPj-0LgeW8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Role of Proper Weight Initialization\n","Proper weight initialization is crucial in training deep neural networks for several reasons:\n","\n","Prevents Vanishing/Exploding Gradients: Good weight initialization ensures that gradients remain within a reasonable range during backpropagation, avoiding issues like vanishing or exploding gradients.\n","\n","Faster Convergence: Properly initialized weights help the network converge faster by starting the optimization process from a good initial point.\n","\n","Stability: It provides stability during training, ensuring that the network learns effectively and reaches an optimal solution.\n","\n","Improved Performance: Proper initialization leads to better overall performance and accuracy of the neural network.\n","\n","In summary, proper weight initialization is essential for effective and efficient training of deep neural networks, ensuring stable gradients, faster convergence, and improved performance."],"metadata":{"id":"L99WAFG7geJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#6.Explain the concept of batch normalization and its impact on weight initialization techniques."],"metadata":{"id":"B2_xAJ6mgtmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Batch Normalization\n","Batch Normalization is a technique used to improve the training of deep neural networks by normalizing the inputs of each layer. It stabilizes and accelerates the learning process.\n","\n","How It Works\n","Normalization: During training, batch normalization normalizes the inputs to each layer so that they have a mean of zero and a variance of one.\n","\n","Scaling and Shifting: It then applies learned scale and shift parameters to the normalized inputs to maintain the representational power of the network.\n","\n","Impact on Weight Initialization\n","Reduces Sensitivity to Initialization: Batch normalization makes the network less sensitive to the initial weights, allowing for more flexibility in choosing weight initialization methods.\n","\n","Improved Training Stability: It helps maintain stable gradients, reducing the impact of vanishing or exploding gradients, even with poor initial weights.\n","\n","Faster Convergence: By maintaining stable distributions of activations, batch normalization speeds up the training process, leading to faster convergence.\n","\n","In summary, batch normalization helps in achieving faster and more stable training of deep neural networks, reducing the dependency on specific weight initialization techniques."],"metadata":{"id":"_P_JK0MdgtjN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#7. Implement He initialization in Python using TensorFlow or PyTorch."],"metadata":{"id":"4Xnnzl9Egtft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Define a model layer with He initialization\n","initializer = tf.keras.initializers.HeNormal()\n","layer = tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer=initializer)\n","\n","# Example usage in a simple model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer=initializer, input_shape=(32,)),\n","    tf.keras.layers.Dense(units=10, activation='softmax')\n","])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIR9toMLgtb1","executionInfo":{"status":"ok","timestamp":1735209022589,"user_tz":-330,"elapsed":6751,"user":{"displayName":"shubhangi belkunde","userId":"13336077163669606462"}},"outputId":"2cb867be-12e6-49b2-8ecd-cd204d64e2f0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}]}]}